#
# Main stack definition for arvine
#

spack_release: v0.15.4

site: scitas

stack_release: "arvine"
stack_version: "v1"

spack_root: /ssoft/spack
spack_external: external

# List of the environments to be managed
environments:
#  - "deneb"
#  - "eltanin"
#  - "fidis"
#  - "gacrux"
#  - "helvetios"
  - "izar"

mirrors:
  local: spack-mirror
  restricted: spack-mirror-restricted

extra_repos:
  scitas-external:
    repo: http://github.com/epfl-scitas/spack-repo-externals.git
    path: scitas-repos-externals/
    tag: releases/arvine
  scitas-spack-packages:
    repo: http://github.com/epfl-scitas/scitas-spack-packages.git
    path: scitas-spack-packages/
    tag: releases/arvine

# default version of compilers mpi and openblas
default_environment:
  os: rhel7
  cpu: intel
  slurm: 20.02.5

  python:
    3: 3.7.7
    2: 2.7.18

  # compiler contains the arch since it is the highest arch this compiler can
  # compile for
  core_compiler: gcc@4.8.5 arch=linux-rhel7-haswell
  compilers: [gcc, intel]
  stack_types: [stable, bleeding_edge]

  stable:
    # intel 19.0 is the highest supported by cuda 10.2.89
    intel:
      compiler: "intel@19.0.5"
      mpi: "intel-mpi@2019.5.281"
      blas: "intel-mkl@2019.5.281"
      suite_version: '2019.5.281'

    # gcc@8.4.0 higest supported by cuda 10.2.89 important for izar
    gcc:
      compiler: "gcc@8.4.0"
      mpi: [ "mvapich2 process_managers=slurm fabrics=mrail threads=multiple" ]
      blas: [ "openblas threads=none" ]

  bleeding_edge:
    gcc:
      # gcc 9 is the highest supported by mvapich2@2.3.4 the current highest
      # version of mvapich2 a stack with gcc 10.2.0 will need a different mpi
      # cuda 11 does not support sm_30 anymmore, gcc+nvptx need to be patched
      # and might be unstable
      compiler: "gcc@9.3.0"
      mpi: [ "mvapich2 process_managers=slurm fabrics=mrail threads=multiple" ]
      blas: [ "openblas threads=none" ]

    intel:
      # intel 19.1.1 is already considered a intel 20 hence the mixed versions
      compiler: "intel@19.1.1"
      mpi: "intel-mpi@2019.7.217"
      blas: "intel-mkl@2020.1.217"
      suite_version: '2020.1.217'

izar:
  gpu: nvidia

  stable:
    gcc:
      compiler: "gcc@8.4.0+nvptx+piclibs ^libiconv"
      mpi: [ "mvapich2 process_managers=slurm fabrics=mrail threads=multiple +cuda" ]

    cuda:
      package: "cuda@10.2.89"
      arch: "sm_70"

  bleeding_edge:
     gcc:
       compiler: "gcc@9.3.0+nvptx+piclibs ^libiconv"
       mpi:
         - "mvapich2 process_managers=slurm fabrics=mrail threads=multiple +cuda"

     cuda:
       package: "cuda@11.0.2"
       arch: "sm_70"

     nvhpc:
       compiler: nvhpc+blas+lapack+mpi
       blas: nvhpc+blas+lapack+mpi
       mpi: nvhpc+blas+lapack+mpi
       version: "20.9"
