#
# Main stack definition for arvine
#

spack_release: v0.15.4

site: scitas

stack_release: "arvine"
stack_version: "v1"

spack_root: /ssoft/spack
spack_external: external

# List of the environments to be managed
environments:
#  - "fidis"
  - "gacrux"
#  - "helvetios" == "gacrux"
#  - "izar"
#  - "gcp-c2"

mirrors:
  local: spack-mirror
  restricted: spack-mirror-restricted

extra_repos:
  scitas-external:
    repo: http://github.com/epfl-scitas/spack-repo-externals.git
    path: scitas-repos-externals/
    tag: releases/arvine
  scitas-spack-packages:
    repo: http://github.com/epfl-scitas/scitas-spack-packages.git
    path: scitas-spack-packages/
    tag: releases/arvine

# default version of compilers mpi and openblas
default_environment:
  os: rhel7.7
  cpu: intel
  slurm: 20.02.5

  python:
    3: 3.7.7
    2: 2.7.18

  # compiler contains the arch since it is the highest arch this compiler can
  # compile for
  core_compiler: gcc@4.8.5 arch=linux-rhel7-haswell
  compilers: [gcc, intel]
  stack_types: [stable, bleeding_edge]

  stable:
    # intel 19.0 is the highest supported by cuda 10.2.89
    intel:
      compiler: "intel@19.0.5"
      mpi: "intel-mpi@2019.5.281"
      blas: "intel-mkl@2019.5.281"
      suite_version: '2019.5.281'

    # gcc@8.4.0 higest supported by cuda 10.2.89 important for izar
    gcc:
      compiler: "gcc@8.4.0"
      mpi: [ "mvapich2 process_managers=slurm fabrics=mrail threads=multiple" ]
      blas: [ "openblas threads=none" ]

  bleeding_edge:
    gcc:
      # gcc 9 is the highest supported by mvapich2@2.3.4 the current highest
      # version of mvapich2 a stack with gcc 10.2.0 will need a different mpi
      # cuda 11 does not support sm_30 anymmore, gcc+nvptx need to be patched
      # and might be unstable
      compiler: "gcc@9.3.0"
      mpi: [ "mvapich2 process_managers=slurm fabrics=mrail threads=multiple" ]
      blas: [ "openblas threads=none" ]

    intel:
      # intel 19.1.1 is already considered a intel 20 hence the mixed versions
      compiler: "intel@19.1.1"
      mpi: "intel-mpi@2019.7.217"
      blas: "intel-mkl@2020.1.217"
      suite_version: '2020.1.217'

izar:
  gpu: nvidia

  compilers: [gcc, intel]

  stable:
    gcc:
      compiler: "gcc@8.4.0+nvptx+piclibs"
      mpi:
        - "mvapich2 process_managers=slurm fabrics=mrail threads=multiple"

    cuda:
      package: "cuda@10.2.89"
      arch: "sm_70"

  bleeding_edge:
     gcc:
       compiler: "gcc@9.3.0+nvptx+piclibs"
       mpi:
         - "mvapich2 process_managers=slurm fabrics=mrail threads=multiple"

     cuda:
       package: "cuda@11.0.2"
       arch: "sm_70"

     # cannot use it as a compiler
     # nvhpc:
     #   compiler: "nvhpc@21.2+blas+lapack+mpi"
     #   blas: "nvhpc@21.2+blas+lapack+mpi"
     #   mpi: "nvhpc@21.2+blas+lapack+mpi"

gcp-c2:
  os: centos7.9
  slurm: 20.02.6
  compilers: [gcc]

  core_compiler: gcc@4.8.5 arch=linux-centos7-haswell
  cloud: gcp

  stable:
    gcc:
      mpi:
        - "mvapich2 process_managers=slurm fabrics=sock threads=multiple"
        - "mvapich2 process_managers=slurm fabrics=sock threads=multiple file_systems=nfs"
        - "mpich+slurm+verbs device=ch4 netmod=ofi pmi=pmi2"
        - "mpich+slurm pmi=pmi2"
        - "openmpi +thread_multiple schedulers=slurm fabrics=auto +pmi"
        - "intel-mpi@2019.7.217"
        - "intel-mpi@2018.4.274"

  bleeding_edge:
     gcc:
       mpi:
         - "mvapich2 process_managers=slurm fabrics=sock threads=multiple"

gacrux:
  slurm: 19.05.3-2

fidis:
  slurm: 19.05.3-2
