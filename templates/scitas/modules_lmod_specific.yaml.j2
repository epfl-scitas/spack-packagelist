################
# COMPILERS
################
{% if not environment.bootstrap %}
  {% for _type in environment.stack_types %}
    {% for _compiler, stack in environment[_type].items() %}
      {% if 'compiler' in stack and 'compiler_prefix' in stack %}
"{{ stack['compiler'] }}":
  environment:
    set:
      CC:  {{ _compiler | compiler_component('cc',  environment, _type) }}
      CXX: {{ _compiler | compiler_component('c++', environment, _type) }}
      F77: {{ _compiler | compiler_component('f77', environment, _type) }}
      FC:  {{ _compiler | compiler_component('f90', environment, _type) }}
      F90: {{ _compiler | compiler_component('f90', environment, _type) }}
        {% if stack['compiler'] == 'nvhpc+blas+lapack+mpi' %}
      SLURM_MPI_TYPE: pmix
      OMPI_MCA_btl_openib_warn_default_gid_prefix: '0'
        {% endif %}
        {% if _compiler == 'intel' or _compiler == 'gcc' or _compiler == 'clang' %}
    prepend_path:
      {% set libdirs = _compiler | compiler_component('libdir', environment, _type) | list_if_not %}
          {% for path in libdirs %}
      LD_LIBRARY_PATH: {{path}}
          {% endfor %}
        {% endif %}
      {% endif %}
    {% endfor %}
  {% endfor %}
{% endif %}

# CUDA specific
'^cuda':
  autoload: direct

nvhpc:
  environment:
    set:
      SLURM_MPI_TYPE: pmix
      OMPI_MCA_btl_openib_warn_default_gid_prefix: '0'

################
# MPI
################
openmpi:
  environment:
    set:
      SLURM_MPI_TYPE: pmi2
      OMPI_MCA_btl_openib_warn_default_gid_prefix: '0'

intel-mpi: # Default for Intel MPI
  environment:
    set:
      IPATH_NO_CPUAFFINITY: '1'
      {% if 'cloud' in environment and environment.cloud == 'gcp' %}
      I_MPI_FABRICS: 'shm:ofi'
      FI_PROVIDER: 'sockets'
      I_MPI_PMI_LIBRARY: /apps/slurm/slurm-{{environment.slurm}}/lib/libpmi.so
      {% else %}
      I_MPI_FABRICS: 'shm:ofa'
      I_MPI_PMI_LIBRARY: /usr/lib64/libpmi.so
      {% endif %}
      {% if environment.name == 'helvetios' or environment.name == 'fidis' %}
      I_MPI_EXTRA_FILESYSTEM: '0'
      # I_MPI_EXTRA_FILESYSTEM_LIST: gpfs
      {% endif %}
      MPICC: mpiicc
      MPICXX: mpiicpc
      MPIF90: mpiifort
      MPIF77: mpiifort
      MPIFC: mpiifort

intel-mpi@2018.4.275: # Default for Intel MPI
  environment:
    set:
      IPATH_NO_CPUAFFINITY: '1'
      {% if 'cloud' in environment and environment.cloud == 'gcp' %}
      I_MPI_FABRICS: 'shm:tcp'
      FI_PROVIDER: 'sockets'
      I_MPI_PMI_LIBRARY: /apps/slurm/slurm-{{environment.slurm}}/lib/libpmi.so
      {% else %}
      I_MPI_FABRICS: 'shm:ofa'
      I_MPI_PMI_LIBRARY: /usr/lib64/libpmi.so
      {% endif %}
      {% if environment.name == 'helvetios' or environment.name == 'fidis' %}
      I_MPI_EXTRA_FILESYSTEM: '1'
      I_MPI_EXTRA_FILESYSTEM_LIST: gpfs
      {% endif %}
      MPICC: mpiicc
      MPICXX: mpiicpc
      MPIF90: mpiifort
      MPIF77: mpiifort
      MPIFC: mpiifort

mvapich2:
  environment:
    set:
      MV2_ON_DEMAND_THRESHOLD: '1'
      {% if environment.name == 'gacrux' or environment.name == 'helvetios' %}
      MV2_IBA_HCA: mlx5_0
      {% endif %}
      {% if environment.name == 'izar' %}
      MV2_IBA_HCA: mlx5_1:mlx5_0
      {% endif %}
      {% if environment.gpu == 'nvidia' %}
      MV2_USE_CUDA: '1'
      {% endif %}

mpich:
  environment:
    set:
      SLURM_MPI_TYPE: pmi2

################
# LAPACK
################
{%if intel in environment.compilers and environment.stable.intel.compiler_prefix %}
intel-mkl:
  environment:
    set:
      MKLROOT: {{ environment.stable.intel.compiler_prefix }}/mkl
      CMAKE_LIBRARY_PATH: {{ environment.stable.intel.compiler_prefix }}/mkl/lib/intel64
{% endif %}

'openblas threads=none':
  environment:
    set:
      'OPENBLAS_NUM_THREADS': '1'

################
# Other software
###############

adf:
  template: 'modules/group_restricted.lua'
  load:
  - 'intel'
  - 'intel-mpi'

comsol:
  environment:
    prepend_path:
      MATLABPATH: '${PREFIX}/mli'

crystal17:
  template: 'modules/group_restricted.lua'

cuda:
  environment:
    set:
      CUDA_LIBRARY: '${PREFIX}/lib64'
      CUDA_INCLUDE: '${PREFIX}/include'

fdtd:
  environment:
    prepend_path:
      PATH: '${PREFIX}/mpich2/nemesis/bin'
      LD_LIBRARY_PATH: '${PREFIX}/mpich2/nemesis/lib'
    set:
      SLURM_MPI_TYPE: pmi2

gaussian:
  template: 'modules/group_restricted.lua'

'hdf5~mpi+cxx':
  environment:
    set:
      HDF5_CC: h5cc
      HDF5_CXX: 'h5c++'
      HDF5_FC: h5fc

'hdf5+mpi~cxx':
  environment:
    set:
      HDF5_CC: h5pcc
      HDF5_FC: h5pfc

metis:
  suffixes:
    ~real64: sp

molpro:
  template: 'modules/group_restricted.lua'

plumed:
  environment:
    set:
      PLUMED_KERNEL: '${PREFIX}/lib/libplumedKernel.so'
    unset:
      - PLUMED_ROOT

py-horovod:
  autoload: direct

py-keras:
  autoload: direct

py-tensorflow:
  autoload: direct

py-theano:
  autoload: direct

py-torch:
  autoload: direct

py-torchvision:
  autoload: direct

# https://bitbucket.org/mpi4py/mpi4py/issues/160/bad-termination-when-running-on-xsede
# MPI4PY uses MPI_THREAD_MULTIPLE in the MPI_Init_thread
'py-mpi4py^mvapich2':
  environment:
    set:
      MV2_ENABLE_AFFINITY: 0

quantum-espresso:
  suffixes:
    'hdf5=parallel': hdf5

scala:
  autoload: 'direct'

scons:
  suffixes:
    '^python@:2.99': py2
    '^python@3:': py3

spark:
  autoload: 'direct'
  environment:
    prepend_path:
      PATH: '/ssoft/spack/scripts/all/spark'

vasp:
  template: 'modules/group_restricted.lua'
